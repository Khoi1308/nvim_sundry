- As you can see on the right, in case of Docker, we have the underlying hardware, infrastructure and then the OS and then Docker installed on the OS. Docker then manages the containers that run with libraries and dependencis alone. In case of VMs, we have the hypervisor like ESX on the hardware, and then the VMs on them. As you can see, each VM has its own OS inside it. Then dependencies and then the application
- The overhead causes higher utilization of underlying resources as there are multiple Virtual Operating Systems and kernel running. The VMs also consumeed higher disk space, as each VM is heavy, and is usually in GBs in size, whereas Docker containers are lightweight and are usually in Mbs in size. This allows Docker containers to boot up faster, usually in a matter of seconds, wheas VMs as we know, takes minutes to boot up as it needs to boot up the entire OS. It's also important to note that Docker has less isolation as more resources, are shared between the containers like kernels, whereas VMs have complete isolation from each other. Since VMs don't rely on the underlying OS or kernel, you can run different types of applications built on different services such as Linux based or Windows based app on the same hypervisor.
- Now, having said that, it's not an either container or VM situation. Its containers and VMs. Now, when you have large environments with 1000s of application containers running on 1000s of Docker host, you will often see containers provisioned on Virtual Docker hosts. That way, we can utilize the advantages of both technologies, we can use the benefits of virtualization, to easily provision or decommision Docker hosts, as required, at the same time make use of the benefits of Docker to easily provision applications and quickly scale them as required. But remember that in this case, we will not be provisioning that many virtual machines as we used to before, because earlier, we provisioned a VM for each application. Now, you might provision a VM for 100s or 1000s of containers. SO how is it done? There are lots of containerized versions of applications readily available as of today. So most organizations have their products contianerized and available in a public Docker repository called Docker Hub or Docker store. 

## Container & Image
- And IMAGE is a package or a template (just like a VM template that you might have worked within the virtualization world), it is used to create on or more containers. Containers are running instances if Images that are isolated and have their own environments and set of processees. As we have seen before, a lot of products have been dockerized already, in case you can't find what you're looking for. You could create your own Image and push it to Docker hub repository, making it available for public. 
- So if you look at it, traditionally, developers developed applications, then they hand it over to operations (Ops) team to deploy and manage it in productive environments. They do that by providing a set of instruction such as information about how the host must be set up, what reprequisites are to be installed on the host, and how the dependencies are to be configured, etc. Since the devops team did not really develop the application on their own, they struggle with setting it up. 
- When they hit an issue, they works with developers to resolve it. With Docker, developers and operations teams work hand in hand to transform the guide into a Docker file with both of their requitements. This Docker file is then used to create an Image for their applications. This Image can now run on any host with Docker installed on it, and is guaranteed to run the same way everywhere. So the Ops team can now simply use the Image to deploy the application. Since the Image was already working, when the developer built it, and operations are have not modified it. It continues to work the same way when deployed in production.
## Docker
- Docker has 2 Editions: Community & Enterprise
 + Enterprise Edition is the certified and supported container platform that comes with enterprise add ons like the Image management, Image security, universal control plane for managing and orchestrating container runtimes. We will discuss more about container orchestration later in this course, and along with some alternatives 
 + Community Edition: I knew that

## Docker command
1. run - start a container
2. ps - list containers
 + docker ps                                     -- List all running container
 + docker ps -a                                  -- List all running or not container
3. stop - stop a container
 + docker stop <container_name>
4. rm - remove a container
 + docker rm <container_name>
5. images - list images
6. rmi - remove images
 + docker rmi <image_name:tag>
7. pull - download an image
 + docker run <image_name:tag>
 + docker pull <image_name>                      -- Pull docker image but not run container
8. exec - execute a command
 + docker exec <container_name> <command_line>   -- Run command line in RUNNING container
9. run - attach & detach
 + docker run -d <image_name>
- When you run a Docker - run command like this, it runs in the foreground or in an attached mode, meaning you will be attached to the console or the standard out of the Docker container. And you will see the output of the web service on your screen. You won't be able to do anything else on this console other than view the output until this Docker container stops. It won't respond to your inputs. Another options is to run the Docker container in the detached mode by providing the slash d option. This will run the Docker container in the background mode, and you will be back to your prompt immediately. The container will continue to run in the BE. 
 + docker attach <container_ID>
- Now if you would like to attach back to the running container later, run the docker attach command and specify the name or ID of the Docker container. Now remember, if you're specifying the ID of a container in any Docker command, you can simply provide the first few characters alone 

10. run - standard input (STDIN)
- Default, the Docker container doesn't listen to a standard input. Even though you're attached to its console, it isn't able to read any input from you. It doesn't have terminal to read inputs from, it runs in a Non Interactive mode. If you'd like to provide your input, you must map the standard input of your host to the Docker container, using "-i" parameter ("-i" parameter is for interactive mode). 
 + docker run -i <image_name:tag>
- When we run the app, at first, it asked us for our name. But when dockerized the prompt is missing, even though it seems to have accepted my input. That is because the application prompt on the terminal and we have not attached to the containers terminal. For this use the "-t" option as well. The "-t" stands for a pseudo terminal. So with the combination of "-it", we're now attached to the terminal, as well as in an interactive mode on the container.
 + docker run -it <image_name:tag>

## Port mapping - port publishing on containers
- Let's go back to the example where we run a simple web application in a Docker container on my Docker host. Remember the underlying host where Docker is installed is called Docker host or Docker engine. When we run a dockerized web application, it run and we're able to see that the server is running. But how does the user access my application? As you can see, my app is listening on port 5000. So I could access my application by using port 5000. But what IP do I use to access it from a web browser? There are 2 options available:
 + Option 1: Use the IP of the Docker container. Every Docker container gets an IP assigned by default (This is internal IP and is only accessible within the Docker host). So if you open a browser from within the Docker host, you can go to "http://IP:PORT" to access the IP address. But since this is an internal IP (Example, 172.17.0.2), users outside of the Docker host cannot access it using this IP. For this, we could use the IP of the Docker host (192.168.1.5). But for that to work, you must have mapped the port inside the Docker container to a free port on the Docker host
  . For example, if I want the users to acces my application through port 80 on my Docker host, I could map port 80 of local host to port 5000 on the Docker container
  . Command: docker run -p <Docker_host>:<Docker_container> <image_name>:<image_tag>

## Volume mapping
Let's now look at how data is persisted in Docker container? When databases and tables are created, the data files are stored in location /var/lib/postgresql/18/ inside the Docker container. Remember, the Docker container has its own isolated file system, and any changes to any files happen within the Docker container. Let's assume you dump a lot of data into the database. What happens if you were delete the PostgreSQL container and remove it? Container along with all the data inside it gets blown away.

If you would like to persisted data, you would want to map a directory outside the container on the Docker host to a directory inside the Docker container.
 + docker run -v <directory_on_host>:<directory_in_contrianer> <image_name>:<tag>

This way when Docker container runs, it will implicitly mount the external directory to a folder inside Docker container. This way all your data will now be stored in the external volume

If you'd like to see additional details about a specific container, 
 + docker inspect <container_name>
 it returns all details of a container in a JSON format, such as the state mounts, configuration data, network settings, etc.

 + docker run -e <command_line> <images_name>
 to set an environment variable within the container. To find the environment variable set on a container that's already running, use Docker inspect command to inspect the properties of a running container. 

# DOCKER IMAGES
 Why would you need to create your own image? 
  - It could either be because you cannot find a component or a service that you want to use as part of your application on Docker hub already, or you & your team decided that the application you're developing will be containerized for ease of shipping and development. 

  - In this case, I'm going to containerize an application a simple web application that I have built using the Python

 First, we need to understand what we are containerizing or what application we are creating an image for? How the application is built? So what by thinking what you might do? If you want to deploy the application manually, we write down the steps required in the right order. I'm creating an image for a simple web application.

 If I were to set it up manually, I would start with an OS like Ubuntu, then update the source repositories using the apt commmand, then install TS dependencies, then copy over the source code of my application to a location like /opt folder, and then finally, run the web server. Now that I have the instructions, create a Dockerfile using this. Here's a quick overview of the process of creating your own image.
  - Create a Docker file named Dockerfile and write down the instructions for setting up your application in it such as installing dependencies, where to copy the source code from and to, what the entrypoint of the application is, etc.

  - Once done, build your image using the Docker buid command and specify the Dockerfile as input as well as a tag name for the image. This will create an image locally on your system.
   + docker build Dockerfile -t <account_name>/<image_name>
   + docker push <account_name>/<image_name>

 - To make it available on the Docker Hub register, run the Docker push command an specify the name of the image you just created. 

 The first line from Ubuntu defines what the base OS should be for this container. Every Docker image must be based off of another image, either an OS or another Image that was creared before based on an OS. Then the copy instruction copies files from the local system onto the Docker image. In this case, the source code of our application is in the current folder, and I'll be copying it over to the location "opt/source-code" inside the docker image. Finally, entrypoint allows us to specify a command that will be run when the image is run as a container. 

 - When Docker builds the images, it builds these in a layered architecture. Each line of instruction creates a new layer in the Docker image with just the changes from the previous layer. 
  For example, the first layer is a base Ubuntu OS, followed by the second instruction that creates a second layer which installs all the apt packages, ...
  docker history <image_name>   -- See more detailed size of each layer

# DOCKER CMD & ENTRYPOINT
 - In this lecture, we will look at commands arguments and entrypoint in Docker.
 When you run the Docker, run Ubuntu command, it runs an instance of Ubuntu image and exit immediately. If you were to list the running containers, you wouldn't see the container running. If you list all containers, including those that are stopped, you will see that the new container you ran is in an exited state. 
 - Why is that? Unlike VMs, containers are not meant to host an OS. Containers are meant to run a specific task or process, such as to host an instance of a web server, or application server, or a databae, or simply to carry out some kind of computation or analysis. Once task is complete, the container exits, a container only lives as long as the process inside ID is alive. If the web service inside the container is stopped or crashes, the container exits.
 - So who defines what process is run within the container. If you look at the Dockerfile for popular Docker images like Ngnix, you will see an instruction called CMD, which stands for command that defines the program that will be run within the container when it starts. For the Ngnix image, it is the Ngnix command. For the MySQL image, it is MySQL command. 
 - What we tried to do earlier was to run a container with a plain Ubuntu OS. Let us look at the Dockerfile for this image. You will see that it uses bash as the default command. Now bash is not really a process like a web server or DB server. It's a shell that listens for inputs from a terminal. If it cannot find the terminal, it exits. When we ran Ubuntu container earlier, Docker created a container from Ubuntu image and launch the bash program. By default, Docker doesn't attach a terminal to a container when it is run. And so the bash program does not find the terminal, and so it exits.
 - Since the process, that was started when the container was created, finished the container exit as well. So how do you specify a different command to start the container?



