- As you can see on the right, in case of Docker, we have the underlying hardware, infrastructure and then the OS and then Docker installed on the OS. Docker then manages the containers that run with libraries and dependencis alone. In case of VMs, we have the hypervisor like ESX on the hardware, and then the VMs on them. As you can see, each VM has its own OS inside it. Then dependencies and then the application
- The overhead causes higher utilization of underlying resources as there are multiple Virtual Operating Systems and kernel running. The VMs also consumeed higher disk space, as each VM is heavy, and is usually in GBs in size, whereas Docker containers are lightweight and are usually in Mbs in size. This allows Docker containers to boot up faster, usually in a matter of seconds, wheas VMs as we know, takes minutes to boot up as it needs to boot up the entire OS. It's also important to note that Docker has less isolation as more resources, are shared between the containers like kernels, whereas VMs have complete isolation from each other. Since VMs don't rely on the underlying OS or kernel, you can run different types of applications built on different services such as Linux based or Windows based app on the same hypervisor.
- Now, having said that, it's not an either container or VM situation. Its containers and VMs. Now, when you have large environments with 1000s of application containers running on 1000s of Docker host, you will often see containers provisioned on Virtual Docker hosts. That way, we can utilize the advantages of both technologies, we can use the benefits of virtualization, to easily provision or decommision Docker hosts, as required, at the same time make use of the benefits of Docker to easily provision applications and quickly scale them as required. But remember that in this case, we will not be provisioning that many virtual machines as we used to before, because earlier, we provisioned a VM for each application. Now, you might provision a VM for 100s or 1000s of containers. SO how is it done? There are lots of containerized versions of applications readily available as of today. So most organizations have their products contianerized and available in a public Docker repository called Docker Hub or Docker store. 

 ## Container & Image
  - And IMAGE is a package or a template (just like a VM template that you might have worked within the virtualization world), it is used to create on or more containers. Containers are running instances if Images that are isolated and have their own environments and set of processees. As we have seen before, a lot of products have been dockerized already, in case you can't find what you're looking for. You could create your own Image and push it to Docker hub repository, making it available for public. 
  - So if you look at it, traditionally, developers developed applications, then they hand it over to operations (Ops) team to deploy and manage it in productive environments. They do that by providing a set of instruction such as information about how the host must be set up, what reprequisites are to be installed on the host, and how the dependencies are to be configured, etc. Since the devops team did not really develop the application on their own, they struggle with setting it up. 
  - When they hit an issue, they works with developers to resolve it. With Docker, developers and operations teams work hand in hand to transform the guide into a Docker file with both of their requitements. This Docker file is then used to create an Image for their applications. This Image can now run on any host with Docker installed on it, and is guaranteed to run the same way everywhere. So the Ops team can now simply use the Image to deploy the application. Since the Image was already working, when the developer built it, and operations are have not modified it. It continues to work the same way when deployed in production.

## Docker
- Docker has 2 Editions: Community & Enterprise
 + Enterprise Edition is the certified and supported container platform that comes with enterprise add ons like the Image management, Image security, universal control plane for managing and orchestrating container runtimes. We will discuss more about container orchestration later in this course, and along with some alternatives 
 + Community Edition: I knew that

## Docker command
1. run - start a container
2. ps - list containers (7)
 + docker ps                                     -- List all running container
 + docker ps -a                                  -- List all running or not container
3. stop - stop a container
 + docker stop <container_name>
4. rm - remove a container
 + docker rm <container_name>
5. images - list images
6. rmi - remove images
 + docker rmi <image_name:tag>
7. pull - download an image
 + docker run <image_name:tag>
 + docker pull <image_name>                      -- Pull docker image but not run container
8. exec - execute a command
 + docker exec <container_name> <command_line>   -- Run command line in RUNNING container
9. run - attach & detach
 + docker run -d <image_name>
- When you run a Docker - run command like this, it runs in the foreground or in an attached mode, meaning you will be attached to the console or the standard out of the Docker container. And you will see the output of the web service on your screen. You won't be able to do anything else on this console other than view the output until this Docker container stops. It won't respond to your inputs. Another options is to run the Docker container in the detached mode by providing the slash d option. This will run the Docker container in the background mode, and you will be back to your prompt immediately. The container will continue to run in the BE. 
 + docker attach <container_ID>
- Now if you would like to attach back to the running container later, run the docker attach command and specify the name or ID of the Docker container. Now remember, if you're specifying the ID of a container in any Docker command, you can simply provide the first few characters alone 

10. run - standard input (STDIN)
- Default, the Docker container doesn't listen to a standard input. Even though you're attached to its console, it isn't able to read any input from you. It doesn't have terminal to read inputs from, it runs in a Non Interactive mode. If you'd like to provide your input, you must map the standard input of your host to the Docker container, using "-i" parameter ("-i" parameter is for interactive mode). 
 + docker run -i <image_name:tag>
- When we run the app, at first, it asked us for our name. But when dockerized the prompt is missing, even though it seems to have accepted my input. That is because the application prompt on the terminal and we have not attached to the containers terminal. For this use the "-t" option as well. The "-t" stands for a pseudo terminal. So with the combination of "-it", we're now attached to the terminal, as well as in an interactive mode on the container.
 + docker run -it <image_name:tag>

## Port mapping - port publishing on containers
- Let's go back to the example where we run a simple web application in a Docker container on my Docker host. Remember the underlying host where Docker is installed is called Docker host or Docker engine. When we run a dockerized web application, it run and we're able to see that the server is running. But how does the user access my application? As you can see, my app is listening on port 5000. So I could access my application by using port 5000. But what IP do I use to access it from a web browser? There are 2 options available:
 + Option 1: Use the IP of the Docker container. Every Docker container gets an IP assigned by default (This is internal IP and is only accessible within the Docker host). So if you open a browser from within the Docker host, you can go to "http://IP:PORT" to access the IP address. But since this is an internal IP (Example, 172.17.0.2), users outside of the Docker host cannot access it using this IP. For this, we could use the IP of the Docker host (192.168.1.5). But for that to work, you must have mapped the port inside the Docker container to a free port on the Docker host
  . For example, if I want the users to acces my application through port 80 on my Docker host, I could map port 80 of local host to port 5000 on the Docker container
  . Command: docker run -p [Docker_host]:[Docker_container] <image_name>:<image_tag>

## Volume mapping
Let's now look at how data is persisted in Docker container? When databases and tables are created, the data files are stored in location /var/lib/postgresql/18/ inside the Docker container. Remember, the Docker container has its own isolated file system, and any changes to any files happen within the Docker container. Let's assume you dump a lot of data into the database. What happens if you were delete the PostgreSQL container and remove it? Container along with all the data inside it gets blown away.

If you would like to persisted data, you would want to map a directory outside the container on the Docker host to a directory inside the Docker container.
 + docker run --mount type=bind,source=[directory_on_host],target=[directory_in_container] [image_name]:[tag] (3)

This way when Docker container runs, it will implicitly mount the external directory to a folder inside Docker container. This way all your data will now be stored in the external volume

If you'd like to see additional details about a specific container, 
 + docker inspect [container_name]/[image_name] (1)
 it returns all details of a container in a JSON format, such as the state mounts, configuration data, network settings, etc.

 + docker run -e <command_line> <images_name>
 to set an environment variable within the container. To find the environment variable set on a container that's already running, use Docker inspect command to inspect the properties of a running container. 

# DOCKER IMAGES
 Why would you need to create your own image? 
  - It could either be because you cannot find a component or a service that you want to use as part of your application on Docker hub already, or you & your team decided that the application you're developing will be containerized for ease of shipping and development. 

  - In this case, I'm going to containerize an application a simple web application that I have built using the Python

 First, we need to understand what we are containerizing or what application we are creating an image for? How the application is built? So what by thinking what you might do? If you want to deploy the application manually, we write down the steps required in the right order. I'm creating an image for a simple web application.

 If I were to set it up manually, I would start with an OS like Ubuntu, then update the source repositories using the apt commmand, then install TS dependencies, then copy over the source code of my application to a location like /opt folder, and then finally, run the web server. Now that I have the instructions, create a Dockerfile using this. Here's a quick overview of the process of creating your own image.
  - Create a Docker file named Dockerfile and write down the instructions for setting up your application in it such as installing dependencies, where to copy the source code from and to, what the entrypoint of the application is, etc.

  - Once done, build your image using the Docker buid command and specify the Dockerfile as input as well as a tag name for the image. This will create an image locally on your system.
   + docker build Dockerfile -t <account_name>/<image_name> (2)
   + docker push <account_name>/<image_name>
   + Typically, image_name is named according to this format: [Register_URL]/[Project_name]/[Service_name]:[Version_tag]

 - To make it available on the Docker Hub register, run the Docker push command an specify the name of the image you just created. 

 The first line from Ubuntu defines what the base OS should be for this container. Every Docker image must be based off of another image, either an OS or another Image that was creared before based on an OS. Then the copy instruction copies files from the local system onto the Docker image. In this case, the source code of our application is in the current folder, and I'll be copying it over to the location "opt/source-code" inside the docker image. Finally, entrypoint allows us to specify a command that will be run when the image is run as a container. 

 - When Docker builds the images, it builds these in a layered architecture. Each line of instruction creates a new layer in the Docker image with just the changes from the previous layer. 
  For example, the first layer is a base Ubuntu OS, followed by the second instruction that creates a second layer which installs all the apt packages, ...
  docker history <image_name>   -- See more detailed size of each layer

# DOCKER CMD & ENTRYPOINT
 - In this lecture, we will look at commands arguments and entrypoint in Docker.
 When you run the Docker, run Ubuntu command, it runs an instance of Ubuntu image and exit immediately. If you were to list the running containers, you wouldn't see the container running. If you list all containers, including those that are stopped, you will see that the new container you ran is in an exited state. 
 - Why is that? 
  + Unlike VM, containers are not meant to host an OS. Containers are meant to run a specific task or process, such as to host an instance of a web server, or application server, or a database, simply to carry out some kind of computation or analysis.
   Once the task is complete, the container exits, a container only lives as long as the process inside ID is alive. If the web service inside the container is stopped or crashes, the container exits. 

  So who defines what process is run within the container?
   If you look at the Dockerfile for popular Docker images like Nginx, you will see an instruction called CMD, which stands for command that defines the program that will be run within the container when it starts.
    For the Nginx image, it is the nginx command. For the MySQL image, it is the MySQL command
   
   What we tried to do earlier was to run a container with a plain Ubuntu OS? Let us look at the Dockerfile for this image, you will see that it uses bash as the default command. Now bash is not really a process like a web server or database server. It is a shell that listens for inputs from a terminal. If it cannot find the terminal, it exits. When we run the Ubuntu container earlier, Docker created a container from the Ubuntu image and launch the bash program. By default, Docker does not attach a terminal to a container when it is run -> and so the bash program does not find the terminal -> and so it exits. Since process that was started when the container was created, finished the container exits as well.

  So how do you specify a different command to start the container?
   + docker run [image_name] [COMMAND]
   - One option is to append a command to the Docker run command. And that way it overrides the default command specified within the images. In this case, I run the Docker run Ubuntu command with command sleep 5 as the added option. 
    + But how do you make that change permanent? Say you want the image to always run the sleep command when it starts -> you will then create your own image from the base Ubuntu image and specify a new command. There are different ways of specifying the command, either the command simply as in a shell form or in a JSON array format. But remember, when you specify in a format, the first element in the array should be the executable  
   - What if I wish to change the number of seconds it sleeps? Currently, it is hardcoded to 5 seconds. As we learned before, one option is to run the Docker run command with the new command appended on it. But it doesn't very good. The name of the image, ubuntu-sleeper in itself implies that the container will sleep, so we shouldn't have to specify the sleep command again. Instead, we would like it to be something like [docker run ubuntu-sleeper 10], we want to pass in the number of seconds that container should sleep and sleep command should be invoked automatically. And that is where the Entrypoint instruction comes into play.
    + The Entrypoint instruction is like the command instruction. As in, you can specify the program that will be run when the container starts and whatever you specify on the command line will get appended to the entrypoint

# DOCKER NETWORKING
 When you install Docker, it creates 3 networks automatically: Bridge, None, and Host. Bridge is the default network a container gets attached to. 
  - If you would like to associate the container with any other network, specify the network information using the network command line parameter
     + docker run [image_name] --network=[network]
  - We will now look at each of these networks. The Bridge network is a private internal network created by Docker on the host. All containers attached to this network by default, and they get an internal IP address, usually in the range 172.17 series. The containers can access each other using this internal IP if required. To access any of these containers from the outside world, map the ports of these containers to port on the Docker host as we have been before. Another way to access the containers externally is to associate the container to the host network, this takes out any network isolation between the Docker host and the Docker container. Meaning if you were to run a web server on port 5000, in a web app container, it is automatically as accessible on the some port externally withour requiring any port mapping as the web container uses the hosts network. 
  - With the None network, the containers are not attached to any network and doesn't have any access to the external network, or other containers. They run in an isolated network. 

 ## User-defined networks
  We just saw the default burst network where the network ID 172.70.0.1. So all containers associated to this default network will be able to communicate to each other. But what if we wish to isolate the containers within the Docker host, for example, 1st-2nd web containers on internal network 172 and the second 2 containers on a different internal network, like 182. By default, Docker only creates 1 internal Bridge network, we could create our own internal network using the command Docker network, create and specify the driver which is bridge in this case, and the subnet of that network followed by the custom isolated network name
     + docker network create --driver bridge --subnet 182.18.0.0/16 [network_name] (6)
     + docker network ls   // list all networks
     + See the network settings and the IP address assigned to an existing container run command (1)
 ## Embedded DNS
  - Containers can reach each other using their name. For example, in this case, I have a web server and a MySQL database container running on the same node
  - How can I get my web server to access the database on the database container? One thing I could do is to use the internal IP address signed to the MySQL container(in this case is 172.17.0.3). But that is not very ideal because it is not guaranteed that the container will get the same IP when the system reboots. The right way to do it is to use the container name. All containers in Docker host can resolve each other with the name of the container. Docker has built in DNS server that helps the containers to resolve each other using the container name. Note that the built in DNS server always runs at Docker address (127.0.0.1).
  - So how does Docker implement networking? What's the technology behind it? How are the containers isolated within the host? 
   + Docker uses network namespaces that creates a seperate namespace for each container. It then uses virtual ethernet pairs to connect containers together. That's all we can talk about for now. More about these are advanced concepts that we discussed in the advanced course on Docker.
   + From this lecture on networking, head over to the pratice test and practice working with networking in Docker

# DOCKER STORAGE
 - We're going to see where and how Docker stores data and how it manages file systems of the containers. Let us start with how Docker stores data on the local file system. 
 - When you install Docker on a system, it creates this folder structure at /var/lib/docker. You have multiple folders under it called aufs, containers, image. volumes, etc. This is where Docker stores all its data by default. When I say data, I mean files related to images and containers running on the Docker host. For example, all files related to containers are stored under the containers folder. And the files related to images are stored under the images folder. A
  ny volumes created by the Docker containers, I created under the volumes folder.
 - How exactly does Docker store the files of an image and a container? To understand that, we need to understand Docker's layered architecture.
 Same image layer is shared by all containers created using this image. If I were to log into the newly created container and say create a new file (temp.txt), it will create that file in the container layer which is read and write. We just said that the files in the image layer are read only meaning you cannot edit anything in those layer. Since we bake our source code into the image, the code is part of the image layer and as such is read only. After running a container, what if I wish to modify the source code to say test to change. Remember, the same image layer may be shared between multiple containers created from this image
 - So does it mean that I cannot modify this file inside the container? I can still modify this file. But before I save the modified file, Docker automatically creates a copy of the file in the read-write layer, and I will then be modifying a different version of the file in the read-write layer. All future modifications will be done on this copy of the file in the read-write layer. This is called copy & write mechanism. So the image will remain the same all the time, until you rebuld the image using the Docker build command (2)
 - What happens when we get rid of the container? all of the data that was store in the container layer also gets deleted, the changes we made to the app.py an the new temp.txt file created will also get removed. So what if we wish to persist this data? For example, if we were working with a database, and we would like to preserve the data created by the container, we could add a persistent volume to the container. To do this, first create a volume using the Docker volume create command
     + docker volume create [volume_name] (4)
  - So when I run the Docker volume create "data_volume" command, it creates a folder called data_volume under the /var/lib/docker/volumes directory. then when I run the Docker container using the Docker run command, I could mount this volume inside the Docker containers rewrite layer using this command 
     + docker run --mount type=volume,source=[volume_name],target=[location_data_inside_container] [image_name] (5)
     + To know the volume'data location inside the container: docker exec [container_name] -it -u [database_username] psql -c "SHOW data_directory;"
 - This will create a new container and mount the data volume that we created into location_data_inside_container. So all data written by the database is in fact stored on the volume created on the Docker host. Even if the container is destroyed, the data still active. 
 - Now what if you didn't run the Docker volume create command (4) before the docker run command (5). For example, if I run the Docker command to create a new instance of Database container with data_volume_2, which I have not created yet, Docker will automatically create a volume named data_volume_2 and mounted to the container. You should be able to see all these volumes if you list the contents of location_data_inside_container. This is called Volume Mounting. As we are mounting a volume created by Docker under the var/lib/docker/volumes folder.
 - But what if we had our data already at another location for example let's say we have some external storage on the Docker host (/data). And we would like to store database data on that volume and not in the default var/lib/docker/volumes folder. In this case, we will run a container using the command (3), we will provide the complete path to the folder we would like to mount that is for /data forwad /MySQL and so it will create a container and mount the folder to the container. This is called Bind Mounting
 - So who is reponsible for doing all of these operations: maintaining the layered architechture, creating a writable layer, moving file across layers to enable, copy & write, etc. It's the storage driver. So Docker uses storage drivers to enable layer architechture. Some of the common storage drivers are AUFS, ZFS, BTRFS, device mapper, overlay, overlay2. The selection of the storage driver depends on the underlying OS being used

# DOCKER COMPOSE
 - Going forward, we will be working with configurations in yaml file, so it is important that you are comfortable with yaml. Let's recap a few things real quick, because we first learned how to run Docker container using the Docker run command. If we needed to setup a complex application running multiple services, a better way to do it is to use Docker compose. With Docker compose, we could create a configuration file in yaml format called docker-compose.yaml and put together the different services and options specific to this to running them in this file. 

 - So let's first get familiarized with the application, because we will be working with the same application in different sections though the rest of this course. 
   + This is a sample voting application which provides an interface for a user to vote and another interface to show the results. The application consists of various components such as the voting app
     . voting-app, which is a web application developed in Python, to provide the user with an interface to choose between 2 options (Cats or Dogs)
     . When you make a selection, the vote is stored in Redis. For those of you, who are new to Redis, in this case serves as a database in memory
     . This vote is then processed by the worker which is an application written in .NET. 
     . The worker application takes the new vote and updates the persistent database, which is PostgreSQL
     . The vote is displayed in a web interface, which is another web application developed in NodeJS. This resulting application rates the count of votes from Postgres database an display it to the user
   + So that is the architechture and data flow of this simple voting application stack.
 - If you run manually all the container by use docker run -d command, we will see that all the instances are running on the host. But there is some problem, it just doesn't seem to work. The problem is that we have successfully run all different containers, but we haven't actually linked together. As in 
   + We haven't told the voting web application to use this particular Redis instance, there could be multiple Redis instance running
   + We haven't told the worker and the resulting app to use this particular Postgres that we ran
 - So how do we do that? That is where we use links. Link is a command line option, which can be used to link 2 containers together
   + For example, the voting-app webservice is dependent on the Redis service. When the web server starts, as you can see, in this piece of code on the web server, it looks for a Redis service running on host Redis. But the voting-app container cannot resolve a host by the name Redis. To make the voting-app aware of the Redis service, we add a link option while running the voting-app container to link it to the Redis container,adding a
     . docker run -d --name=[new_container_name] [port_docker_host]:[port_container] --link [container_name_link]:[new_container_link] [image_name]
     . The above command is deprecated, you can use network command instead. Firt you create Network use this command (6), then run the containers that you want to link together use --network [network_name] [image_name]
   + What this is in fact doing is? it creates an entry into the etc host file on the voting-app container, adding an entry with the host name "redis" with the internal IP of the Redis container. Similarily, we add a link for the result-app to communicate with the database by adding a link option to refer the database by the name "db"

 - Then we could simply run a Docker compose up command to bring up the entire application stack is easier to implement, run and maintain as all changes are always stored in the Docker compose configuration file. However, this is all only applicable to running containers on a single Docker host. And for now, don't worry about the yaml file, we will take a closer look at the yaml file in a bit and see how to put it together. That was a really simple application that I put together. Let us look at a better example, i'm going to use the same sample application that everyone uses to demonstrate Docker. It's a simple yet comprehensive application developed
 - Generate docker-compose file:
   + Once we have the Docker run commands tested and ready, it is easy to generate a Docker compose file from it. We start by creating a dictionary of container names, we will use the same name we used in the Docker run commands. So we take all the names and create a key with each of them 
   + Then under each item, we specify which image to use. The key is image: [image_name] [1]
   + Next, inspect the commands and see what are the other options used, let's move those ports under the respective containers. 
   + Next, we are left with links. 
   + Finally use run docker-compose up command to bring up the entire application stack.
 - If we would like to instruct docker-compose to run a docker build, instead of trying to pull a image, we can replace the image line [1] with a built line and specify the location of a directory, which contains the application code, and a Docker file with instructions to build the Docker Image
 
 - Back in our docker-compose file, note that I have actually stripped out the port section for simplicity's sake, there's still there, but they're just not shown here. The first thing we need to do if we're to use networks is to define the networks we're going to use. In our case, we have 2 networks: front-end, and back-end.
 - So create a new property called networks at the root level, and listen the services in the docker-compose file, and add a map of networks we're planning to use. Then under each service, create a network's property and provide a list of networks that service must be attached to. In case of Redis and DB, it's only the back-end network. In case of the front-end applications such as voting app and the result app, they're required to be a test to both a front-end and a back-end network.
 - You must also add section for worker container to be added to the back-end network. I have just omitted that in this slide due to space constrainets. Now that you have seen docker-compose file, head over to the coding exercises and practice developing some docker-compose file 

# DOCKER REGISTRY
  - Docker registry, Github container registry, harbor

# DOCKER ENGINE
  - How it actually runs applications in isolated containers? How it works under the hood?
  - Docker engine as we have learned before you simply refer to a host with Docker installed on it. When you install Docker on a Linux host, you're actually installing 3 different component: Docker Deamon, Rest API, Docker CLI
    + Docker Deamon is background process that manages Docker object such as Images, Containers, Volumes and Networks
    + Docker Rest API server is the API interface that programs can use to talk to the Deamon and provide instructions. You could create your own tools using this Rest API.
    + Docker CLI is nothing but the command line interface that we're been using, until now to perform actions such as running a containers, stopping containers, destroying images, etc. It uses the Rest API to interact with the Docker Deamon.
  - Something to note here is that the Docker CLI need not necessarily be on the same host. It could be on another system like a laptop, and can still worl with a remote Docker engine
    + To run container outside localhost: docker -H=remote-docker-engine_address:[PORT] [image_name]
  - Now let's try and understand how exactly our applications containerized in Docker? How does it work under the hood? Docker use namespaces to isolate workspace: process IDs (PID), Network, Inter process communication (IPC), Mount, Unix timesharing systems (UTS) are created in their own namespace, thereby providing isolation between containers. 
  - Let's take a look at one of the namespace isolation technique
## Namespace - PID
  - Whenever a Linux system boots up, it starts with just 1 process with a process ID of one. This is the root process and kicks off all the other processes in the system.
  - By the time, the system boots up completely, we have a handful of processes running. This can be seen by running, the <ps> command to list all the running processes. The process IDs are unique and 2 processes cannot have the same process ID
  - Now if we were to create a container, which is basically like a child system within the current system, the child system needs to think that it is an independent system on its own. And it has its own set of processes originating from a root process with a process ID of one
  - But we know there is no hard isolation between the containers and the underlying host. So the processes running inside the containerare in fact processes running on the underlying host. And so the 2 processes cannot have the same process ID of one. This is where namespaces come into play. With process ID namespaces, each process can have multiple process IDs associated with it.
  - For example, when the processes start in the container, it's actually just another set of processes on the bas Linux system, and it gets the next available process ID (in this case, maybe 5 and six). However, they also get another process ID starting with PID 1 in the container namespace which is only visible inside the container. So the container thinks that it has its own route process tree. And so it is an independent system.
  - So how does that relate to an actual system? How do you see this on host? Let's I say I were to run an Nginx server as a container. We know that the Nginx container runs and Nginx service. If we were to list all the services inside the Docker container, we see that Nginx service running with a process ID of one. This is the process ID of the service inside of the container namespace. If we list the services on the Docker host, we will see the same service but with a different PID that indicates that all processes are in fact running on the same host but seperated into their own containers using namespace. 
  - So we learned that the underlying Docker host as well as the containers share the same system resources such as CPU and Memory. How much of the resources are dedicated to the host and the containers? And how does Docker manage and share resources between containers? By default, there is no restriction as to how much of a resource a container can use. And hence, a container may end up utilizing all of the resources on the underlying host. But there is a way to restrict the amount of CPU or Memory a container can use. Docker uses cgroups or control group to restrict the amount of hardware resources allocated to each container.
    + docker run --cpus=value [image_name]     // Limits usage to <value>% of a single CPU core
    + docker run --memory=value [image_name]   // Limits the amount of memory the container can use


