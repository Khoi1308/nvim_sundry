- As you can see on the right, in case of Docker, we have the underlying hardware, infrastructure and then the OS and then Docker installed on the OS. Docker then manages the containers that run with libraries and dependencis alone. In case of VMs, we have the hypervisor like ESX on the hardware, and then the VMs on them. As you can see, each VM has its own OS inside it. Then dependencies and then the application
- The overhead causes higher utilization of underlying resources as there are multiple Virtual Operating Systems and kernel running. The VMs also consumeed higher disk space, as each VM is heavy, and is usually in GBs in size, whereas Docker containers are lightweight and are usually in Mbs in size. This allows Docker containers to boot up faster, usually in a matter of seconds, wheas VMs as we know, takes minutes to boot up as it needs to boot up the entire OS. It's also important to note that Docker has less isolation as more resources, are shared between the containers like kernels, whereas VMs have complete isolation from each other. Since VMs don't rely on the underlying OS or kernel, you can run different types of applications built on different services such as Linux based or Windows based app on the same hypervisor.
- Now, having said that, it's not an either container or VM situation. Its containers and VMs. Now, when you have large environments with 1000s of application containers running on 1000s of Docker host, you will often see containers provisioned on Virtual Docker hosts. That way, we can utilize the advantages of both technologies, we can use the benefits of virtualization, to easily provision or decommision Docker hosts, as required, at the same time make use of the benefits of Docker to easily provision applications and quickly scale them as required. But remember that in this case, we will not be provisioning that many virtual machines as we used to before, because earlier, we provisioned a VM for each application. Now, you might provision a VM for 100s or 1000s of containers. SO how is it done? There are lots of containerized versions of applications readily available as of today. So most organizations have their products contianerized and available in a public Docker repository called Docker Hub or Docker store. 

## Container & Image
- And IMAGE is a package or a template (just like a VM template that you might have worked within the virtualization world), it is used to create on or more containers. Containers are running instances if Images that are isolated and have their own environments and set of processees. As we have seen before, a lot of products have been dockerized already, in case you can't find what you're looking for. You could create your own Image and push it to Docker hub repository, making it available for public. 
- So if you look at it, traditionally, developers developed applications, then they hand it over to operations (Ops) team to deploy and manage it in productive environments. They do that by providing a set of instruction such as information about how the host must be set up, what reprequisites are to be installed on the host, and how the dependencies are to be configured, etc. Since the devops team did not really develop the application on their own, they struggle with setting it up. 
- When they hit an issue, they works with developers to resolve it. With Docker, developers and operations teams work hand in hand to transform the guide into a Docker file with both of their requitements. This Docker file is then used to create an Image for their applications. This Image can now run on any host with Docker installed on it, and is guaranteed to run the same way everywhere. So the Ops team can now simply use the Image to deploy the application. Since the Image was already working, when the developer built it, and operations are have not modified it. It continues to work the same way when deployed in production.
## Docker
- Docker has 2 Editions: Community & Enterprise
 + Enterprise Edition is the certified and supported container platform that comes with enterprise add ons like the Image management, Image security, universal control plane for managing and orchestrating container runtimes. We will discuss more about container orchestration later in this course, and along with some alternatives 
 + Community Edition: I knew that

## Docker command
1. run - start a container
2. ps - list containers
 + docker ps                                     -- List all running container
 + docker ps -a                                  -- List all running or not container
3. stop - stop a container
 + docker stop <container_name>
4. rm - remove a container
 + docker rm <container_name>
5. images - list images
6. rmi - remove images
 + docker rmi <image_name:tag>
7. pull - download an image
 + docker run <image_name:tag>
 + docker pull <image_name>                      -- Pull docker image but not run container
8. exec - execute a command
 + docker exec <container_name> <command_line>   -- Run command line in RUNNING container
9. run - attach & detach
 + docker run -d <image_name>
- When you run a Docker - run command like this, it runs in the foreground or in an attached mode, meaning you will be attached to the console or the standard out of the Docker container. And you will see the output of the web service on your screen. You won't be able to do anything else on this console other than view the output until this Docker container stops. It won't respond to your inputs. Another options is to run the Docker container in the detached mode by providing the slash d option. This will run the Docker container in the background mode, and you will be back to your prompt immediately. The container will continue to run in the BE. 
 + docker attach <container_ID>
- Now if you would like to attach back to the running container later, run the docker attach command and specify the name or ID of the Docker container. Now remember, if you're specifying the ID of a container in any Docker command, you can simply provide the first few characters alone 

10. run - standard input (STDIN)
- Default, the Docker container doesn't listen to a standard input. Even though you're attached to its console, it isn't able to read any input from you. It doesn't have terminal to read inputs from, it runs in a Non Interactive mode. If you'd like to provide your input, you must map the standard input of your host to the Docker container, using "-i" parameter ("-i" parameter is for interactive mode). 
 + docker run -i <image_name:tag>
- When we run the app, at first, it asked us for our name. But when dockerized the prompt is missing, even though it seems to have accepted my input. That is because the application prompt on the terminal and we have not attached to the containers terminal. For this use the "-t" option as well. The "-t" stands for a pseudo terminal. So with the combination of "-it", we're now attached to the terminal, as well as in an interactive mode on the container.
 + docker run -it <image_name:tag>

## Port mapping - port publishing on containers
- Let's go back to the example where we run a simple web application in a Docker container on my Docker host. Remember the underlying host where Docker is installed is called Docker host or Docker engine. When we run a dockerized web application, it run and we're able to see that the server is running. But how does the user access my application? As you can see, my app is listening on port 5000. So I could access my application by using port 5000. But what IP do I use to access it from a web browser? There are 2 options available:
 + Option 1: Use the IP of the Docker container. Every Docker container gets an IP assigned by default (This is internal IP and is only accessible within the Docker host). So if you open a browser from within the Docker host, you can go to "http://IP:PORT" to access the IP address. But since this is an internal IP (Example, 172.17.0.2), users outside of the Docker host cannot access it using this IP. For this, we could use the IP of the Docker host (192.168.1.5). But for that to work, you must have mapped the port inside the Docker container to a free port on the Docker host
  . For example, if I want the users to acces my application through port 80 on my Docker host, I could map port 80 of local host to port 5000 on the Docker container
  . Command: docker run -p <Docker_host>:<Docker_container> <image_name>:<image_tag>

## Volume mapping
Let's now look at how data is persisted in Docker container? When databases and tables are created, the data files are stored in location /var/lib/postgresql/18/ inside the Docker container. Remember, the Docker container has its own isolated file system, and any changes to any files happen within the Docker container. Let's assume you dump a lot of data into the database. What happens if you were delete the PostgreSQL container and remove it? Container along with all the data inside it gets blown away.

If you would like to persisted data, you would want to map a directory outside the container on the Docker host to a directory inside the container.
 + docker run -v <directory_on_host>:<directory_in_contrianer> <image_name>:<tag>






